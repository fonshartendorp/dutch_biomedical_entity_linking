{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erMGoxGFM-Cd",
        "outputId": "10ea0521-165a-4958-8143-5cf76d401c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-metric-learning in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-metric-learning\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtzGeiFqMmmD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast\n",
        "from torch.cuda.amp import GradScaler\n",
        "from pytorch_metric_learning import samplers\n",
        "import logging\n",
        "import time\n",
        "import pdb\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtY8DwFVM0Le"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/sapBERT-DUT-cambridge\")\n",
        "# finetune_dataset = 'nl_wiki_bel_all'\n",
        "finetune_dataset = 'nl_wiki_bel_traintestsplit'\n",
        "sapBERT_epochs = 1\n",
        "finetune_epochs = 10\n",
        "input_model_directory_path = f'/content/drive/MyDrive/sapBERT-DUT-cambridge/results/medRoBERTa_sapBERT/{sapBERT_epochs}_epoch/ft_0_epoch/model'\n",
        "output_model_directory_path = f'/content/drive/MyDrive/sapBERT-DUT-cambridge/results/medRoBERTa_sapBERT/{sapBERT_epochs}_epoch/ft_{finetune_epochs}_epoch/{finetune_dataset}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9qk5bBgNI25"
      },
      "outputs": [],
      "source": [
        "from src.data_loader import (\n",
        "    DictionaryDataset,\n",
        "    QueryDataset,\n",
        "    QueryDataset_pretraining,\n",
        "    MetricLearningDataset,\n",
        "    MetricLearningDataset_pairwise,\n",
        ")\n",
        "from src.model_wrapper import (\n",
        "    Model_Wrapper\n",
        ")\n",
        "from src.metric_learning import (\n",
        "    Sap_Metric_Learning,\n",
        ")\n",
        "\n",
        "LOGGER = logging.getLogger()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxYnZi-rQZuK"
      },
      "outputs": [],
      "source": [
        "datasets = {\n",
        "    'nl_wiki_bel_all' : '/content/drive/MyDrive/sapBERT-DUT-cambridge/training_data/nl-wiki-bel_finetune_dataset_all.txt',\n",
        "    'nl_wiki_bel_traintestsplit' : '/content/drive/MyDrive/sapBERT-DUT-cambridge/training_data/nl-wiki-bel_finetune_dataset_train-split.txt'\n",
        "}\n",
        "\n",
        "args = {\n",
        "    'train_dir' : datasets[finetune_dataset],\n",
        "    # 'train_dir' : f'/content/drive/MyDrive/sapBERT-DUT-cambridge/training_data/nl-wiki-bel_finetune_dataset_train-split.txt',\n",
        "    # 'train_dir' : '/content/drive/MyDrive/sapBERT-DUT-cambridge/training_data/nl-wiki-bel_finetune_dataset_all.txt',\n",
        "    # 'model_dir' : 'CLTL/MedRoBERTa.nl',\n",
        "    'model_dir' : input_model_directory_path,\n",
        "    'output_dir' : output_model_directory_path,\n",
        "    'use_cuda' : True,\n",
        "    'learning_rate' : 0.0001,\n",
        "    'weight_decay' : 0.01,\n",
        "    'max_length' : 25,\n",
        "    'train_batch_size' : 512,\n",
        "    'epoch' : finetune_epochs,\n",
        "    'checkpoint_step' : 10000000,\n",
        "    'pairwise' : True,\n",
        "    'amp' : True,\n",
        "    'random_seed' : 1993,\n",
        "    'loss' : 'ms_loss',\n",
        "    'use_miner' : True,\n",
        "    'miner_margin' : 0.2,\n",
        "    'type_of_triplets' : 'all',\n",
        "    'agg_mode' : 'cls',\n",
        "    'save_checkpoint_all' : True\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evBrm6QWOHgc"
      },
      "outputs": [],
      "source": [
        "def init_logging():\n",
        "    LOGGER.setLevel(logging.INFO)\n",
        "    fmt = logging.Formatter('%(asctime)s: [ %(message)s ]',\n",
        "                            '%m/%d/%Y %I:%M:%S %p')\n",
        "    console = logging.StreamHandler()\n",
        "    console.setFormatter(fmt)\n",
        "    LOGGER.addHandler(console)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mttHiZ1KOJeE"
      },
      "outputs": [],
      "source": [
        "def init_seed(seed=None):\n",
        "    if seed is None:\n",
        "        seed = int(round(time.time() * 1000)) % 10000\n",
        "\n",
        "    LOGGER.info(\"Using seed={}, pid={}\".format(seed, os.getpid()))\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl-gHDzQOL0U"
      },
      "outputs": [],
      "source": [
        "def load_dictionary(dictionary_path):\n",
        "    \"\"\"\n",
        "    load dictionary\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dictionary_path : str\n",
        "        a path of dictionary\n",
        "    \"\"\"\n",
        "    dictionary = DictionaryDataset(\n",
        "            dictionary_path = dictionary_path\n",
        "    )\n",
        "\n",
        "    return dictionary.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Fmma7FaOYqw"
      },
      "outputs": [],
      "source": [
        "def load_queries(data_dir, filter_composite, filter_duplicate):\n",
        "    \"\"\"\n",
        "    load query data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    is_train : bool\n",
        "        train or dev\n",
        "    filter_composite : bool\n",
        "        filter composite mentions\n",
        "    filter_duplicate : bool\n",
        "        filter duplicate queries\n",
        "    \"\"\"\n",
        "    dataset = QueryDataset(\n",
        "        data_dir=data_dir,\n",
        "        filter_composite=filter_composite,\n",
        "        filter_duplicate=filter_duplicate\n",
        "    )\n",
        "\n",
        "    return dataset.data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOaq0wZRObP4"
      },
      "outputs": [],
      "source": [
        "def load_queries_pretraining(data_dir, filter_duplicate):\n",
        "    \"\"\"\n",
        "    load query data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    is_train : bool\n",
        "        train or dev\n",
        "    filter_composite : bool\n",
        "        filter composite mentions\n",
        "    filter_duplicate : bool\n",
        "        filter duplicate queries\n",
        "    \"\"\"\n",
        "    dataset = QueryDataset_pretraining(\n",
        "        data_dir=data_dir,\n",
        "        filter_duplicate=filter_duplicate\n",
        "    )\n",
        "\n",
        "    return dataset.data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HUORGJPOfUr"
      },
      "outputs": [],
      "source": [
        "def train(args, data_loader, model, scaler=None, model_wrapper=None, step_global=0):\n",
        "    LOGGER.info(\"train!\")\n",
        "\n",
        "    train_loss = 0\n",
        "    train_steps = 0\n",
        "    model.cuda()\n",
        "    model.train()\n",
        "    for i, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "        model.optimizer.zero_grad()\n",
        "\n",
        "        batch_x1, batch_x2, batch_y = data\n",
        "        batch_x_cuda1, batch_x_cuda2 = {},{}\n",
        "        for k,v in batch_x1.items():\n",
        "            batch_x_cuda1[k] = v.cuda()\n",
        "        for k,v in batch_x2.items():\n",
        "            batch_x_cuda2[k] = v.cuda()\n",
        "\n",
        "        batch_y_cuda = batch_y.cuda()\n",
        "\n",
        "        if args['amp']:\n",
        "            with autocast():\n",
        "                loss = model(batch_x_cuda1, batch_x_cuda2, batch_y_cuda)\n",
        "        else:\n",
        "            loss = model(batch_x_cuda1, batch_x_cuda2, batch_y_cuda)\n",
        "        if args['amp']:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(model.optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            model.optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        # wandb.log({\"Loss\": loss.item()})\n",
        "        train_steps += 1\n",
        "        step_global += 1\n",
        "        #if (i+1) % 10 == 0:\n",
        "        #LOGGER.info (\"epoch: {} loss: {:.3f}\".format(i+1,train_loss / (train_steps+1e-9)))\n",
        "        #LOGGER.info (\"epoch: {} loss: {:.3f}\".format(i+1, loss.item()))\n",
        "\n",
        "        # save model every K iterations\n",
        "        if step_global % args['checkpoint_step'] == 0:\n",
        "            checkpoint_dir = os.path.join(args['output_dir'], \"checkpoint_iter_{}\".format(str(step_global)))\n",
        "            if not os.path.exists(checkpoint_dir):\n",
        "                os.makedirs(checkpoint_dir)\n",
        "            model_wrapper.save_model(checkpoint_dir)\n",
        "    train_loss /= (train_steps + 1e-9)\n",
        "    return train_loss, step_global\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LKKwXXyO3Zu"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    init_logging()\n",
        "\n",
        "    torch.manual_seed(args['random_seed'])\n",
        "\n",
        "    # prepare for output\n",
        "    if not os.path.exists(args['output_dir']):\n",
        "        os.makedirs(args['output_dir'])\n",
        "\n",
        "\n",
        "    # load BERT tokenizer, dense_encoder\n",
        "    model_wrapper = Model_Wrapper()\n",
        "\n",
        "    encoder, tokenizer = model_wrapper.load_bert(\n",
        "        # path='/content/drive/MyDrive/sapBERT-DUT-cambridge/output_dutch-umls_second-phase_1-epoch',\n",
        "        path=args['model_dir'],\n",
        "        max_length=25,\n",
        "        use_cuda=True,\n",
        "    )\n",
        "\n",
        "    # load SAP model\n",
        "    model = Sap_Metric_Learning(\n",
        "            encoder = encoder,\n",
        "            learning_rate=args['learning_rate'],\n",
        "            weight_decay=args['weight_decay'],\n",
        "            use_cuda=args['use_cuda'],\n",
        "            pairwise=args['pairwise'],\n",
        "            loss=args['loss'],\n",
        "            use_miner=args['use_miner'],\n",
        "            miner_margin=args['miner_margin'],\n",
        "            type_of_triplets=args['type_of_triplets'],\n",
        "            agg_mode=args['agg_mode'],\n",
        "    )\n",
        "\n",
        "    def collate_fn_batch_encoding(batch):\n",
        "        query1, query2, query_id = zip(*batch)\n",
        "        query_encodings1 = tokenizer.batch_encode_plus(\n",
        "                list(query1),\n",
        "                max_length=args['max_length'],\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                add_special_tokens=True,\n",
        "                return_tensors=\"pt\")\n",
        "        query_encodings2 = tokenizer.batch_encode_plus(\n",
        "                list(query2),\n",
        "                max_length=args['max_length'],\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                add_special_tokens=True,\n",
        "                return_tensors=\"pt\")\n",
        "        #query_encodings_cuda = {}\n",
        "        #for k,v in query_encodings.items():\n",
        "        #    query_encodings_cuda[k] = v.cuda()\n",
        "        query_ids = torch.tensor(list(query_id))\n",
        "        return  query_encodings1, query_encodings2, query_ids\n",
        "\n",
        "    if args['pairwise']:\n",
        "        train_set = MetricLearningDataset_pairwise(\n",
        "                path=args['train_dir'],\n",
        "                tokenizer = tokenizer\n",
        "        )\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_set,\n",
        "            batch_size=args['train_batch_size'],\n",
        "            shuffle=True,\n",
        "            num_workers=16,\n",
        "            collate_fn=collate_fn_batch_encoding\n",
        "        )\n",
        "    else:\n",
        "        train_set = MetricLearningDataset(\n",
        "            path=args['train_dir'],\n",
        "            tokenizer = tokenizer\n",
        "        )\n",
        "        # using a sampler\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_set,\n",
        "            batch_size=args['train_batch_size'],\n",
        "            #shuffle=True,\n",
        "            sampler=samplers.MPerClassSampler(train_set.query_ids,\\\n",
        "                2, length_before_new_iter=100000),\n",
        "            num_workers=16,\n",
        "            )\n",
        "    # mixed precision training\n",
        "    if args['amp']:\n",
        "        scaler = GradScaler()\n",
        "    else:\n",
        "        scaler = None\n",
        "\n",
        "    start = time.time()\n",
        "    step_global = 0\n",
        "    for epoch in range(1,args['epoch']+1):\n",
        "        # embed dense representations for query and dictionary for train\n",
        "        # Important! This is iterative process because dense represenation changes as model is trained.\n",
        "        LOGGER.info(\"Epoch {}/{}\".format(epoch,args['epoch']))\n",
        "\n",
        "        # train\n",
        "        train_loss, step_global = train(args, data_loader=train_loader, model=model, scaler=scaler, model_wrapper=model_wrapper, step_global=step_global)\n",
        "        LOGGER.info('loss/train_per_epoch={}/{}'.format(train_loss,epoch))\n",
        "\n",
        "        # save model every epoch\n",
        "        if args['save_checkpoint_all']:\n",
        "            checkpoint_dir = os.path.join(args['output_dir'], \"checkpoint_{}\".format(epoch))\n",
        "            if not os.path.exists(checkpoint_dir):\n",
        "                os.makedirs(checkpoint_dir)\n",
        "            model_wrapper.save_model(checkpoint_dir)\n",
        "\n",
        "        # save model last epoch\n",
        "        if epoch == args['epoch']:\n",
        "            model_wrapper.save_model(args['output_dir'])\n",
        "\n",
        "    end = time.time()\n",
        "    training_time = end-start\n",
        "    training_hour = int(training_time/60/60)\n",
        "    training_minute = int(training_time/60 % 60)\n",
        "    training_second = int(training_time % 60)\n",
        "    LOGGER.info(\"Training Time!{} hours {} minutes {} seconds\".format(training_hour, training_minute, training_second))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZcl7nZ-O4yJ",
        "outputId": "3d3dd00e-c079-4728-dd29-68d3bf8b9767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train_dir': '/content/drive/MyDrive/sapBERT-DUT-cambridge/training_data/nl-wiki-bel_finetune_dataset_train-split.txt', 'model_dir': '/content/drive/MyDrive/sapBERT-DUT-cambridge/results/medRoBERTa_sapBERT/1_epoch/ft_0_epoch/model', 'output_dir': '/content/drive/MyDrive/sapBERT-DUT-cambridge/results/medRoBERTa_sapBERT/1_epoch/ft_10_epoch/nl_wiki_bel_traintestsplit', 'use_cuda': True, 'learning_rate': 0.0001, 'weight_decay': 0.01, 'max_length': 25, 'train_batch_size': 512, 'epoch': 10, 'checkpoint_step': 10000000, 'pairwise': True, 'amp': True, 'random_seed': 1993, 'loss': 'ms_loss', 'use_miner': True, 'miner_margin': 0.2, 'type_of_triplets': 'all', 'agg_mode': 'cls', 'save_checkpoint_all': True}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:src.metric_learning:Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "10/24/2023 11:46:28 AM: [ Sap_Metric_Learning! learning_rate=0.0001 weight_decay=0.01 use_cuda=True loss=ms_loss use_miner=True miner_margin=0.2 type_of_triplets=all agg_mode=cls ]\n",
            "INFO:root:Epoch 1/10\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "10/24/2023 11:46:28 AM: [ Epoch 1/10 ]\n",
            "INFO:root:train!\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n",
            "10/24/2023 11:46:28 AM: [ train! ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "miner: TripletMarginMiner(\n",
            "  (distance): LpDistance()\n",
            ")\n",
            "loss: MultiSimilarityLoss(\n",
            "  (distance): CosineSimilarity()\n",
            "  (reducer): MeanReducer()\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:01<00:00,  2.39it/s]\n",
            "INFO:root:loss/train_per_epoch=1.0522269454948092/1\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "10/24/2023 11:46:30 AM: [ loss/train_per_epoch=1.0522269454948092/1 ]\n",
            "INFO:root:Epoch 2/10\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "10/24/2023 11:46:32 AM: [ Epoch 2/10 ]\n",
            "INFO:root:train!\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "10/24/2023 11:46:32 AM: [ train! ]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.45it/s]\n",
            "INFO:root:loss/train_per_epoch=1.0087262389522251/2\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "10/24/2023 11:46:34 AM: [ loss/train_per_epoch=1.0087262389522251/2 ]\n",
            "INFO:root:Epoch 3/10\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "10/24/2023 11:46:35 AM: [ Epoch 3/10 ]\n",
            "INFO:root:train!\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "10/24/2023 11:46:35 AM: [ train! ]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.49it/s]\n",
            "INFO:root:loss/train_per_epoch=0.9511026439673177/3\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "10/24/2023 11:46:38 AM: [ loss/train_per_epoch=0.9511026439673177/3 ]\n",
            "INFO:root:Epoch 4/10\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "10/24/2023 11:46:41 AM: [ Epoch 4/10 ]\n",
            "INFO:root:train!\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "10/24/2023 11:46:41 AM: [ train! ]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.35it/s]\n",
            "INFO:root:loss/train_per_epoch=0.9010924545657959/4\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "10/24/2023 11:46:43 AM: [ loss/train_per_epoch=0.9010924545657959/4 ]\n",
            "INFO:root:Epoch 5/10\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "10/24/2023 11:46:45 AM: [ Epoch 5/10 ]\n",
            "INFO:root:train!\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "10/24/2023 11:46:45 AM: [ train! ]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.39it/s]\n",
            "INFO:root:loss/train_per_epoch=0.8502801058550874/5\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "10/24/2023 11:46:47 AM: [ loss/train_per_epoch=0.8502801058550874/5 ]\n",
            "INFO:root:Epoch 6/10\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "10/24/2023 11:46:49 AM: [ Epoch 6/10 ]\n",
            "INFO:root:train!\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "10/24/2023 11:46:49 AM: [ train! ]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.41it/s]\n",
            "INFO:root:loss/train_per_epoch=0.8131113050335386/6\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "10/24/2023 11:46:51 AM: [ loss/train_per_epoch=0.8131113050335386/6 ]\n",
            "INFO:root:Epoch 7/10\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "10/24/2023 11:46:53 AM: [ Epoch 7/10 ]\n",
            "INFO:root:train!\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "10/24/2023 11:46:53 AM: [ train! ]\n",
            "100%|██████████| 4/4 [00:02<00:00,  1.91it/s]\n",
            "INFO:root:loss/train_per_epoch=0.7811190185978263/7\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "10/24/2023 11:46:56 AM: [ loss/train_per_epoch=0.7811190185978263/7 ]\n",
            "INFO:root:Epoch 8/10\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "10/24/2023 11:46:57 AM: [ Epoch 8/10 ]\n",
            "INFO:root:train!\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "10/24/2023 11:46:57 AM: [ train! ]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.32it/s]\n",
            "INFO:root:loss/train_per_epoch=0.7279594389764498/8\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "10/24/2023 11:47:00 AM: [ loss/train_per_epoch=0.7279594389764498/8 ]\n",
            "INFO:root:Epoch 9/10\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "10/24/2023 11:47:02 AM: [ Epoch 9/10 ]\n",
            "INFO:root:train!\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "10/24/2023 11:47:02 AM: [ train! ]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.32it/s]\n",
            "INFO:root:loss/train_per_epoch=0.6891404835647067/9\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "10/24/2023 11:47:05 AM: [ loss/train_per_epoch=0.6891404835647067/9 ]\n",
            "INFO:root:Epoch 10/10\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "10/24/2023 11:47:06 AM: [ Epoch 10/10 ]\n",
            "INFO:root:train!\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "10/24/2023 11:47:06 AM: [ train! ]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.38it/s]\n",
            "INFO:root:loss/train_per_epoch=0.6927075533327206/10\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "10/24/2023 11:47:09 AM: [ loss/train_per_epoch=0.6927075533327206/10 ]\n",
            "INFO:root:Training Time!0 hours 0 minutes 44 seconds\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n",
            "10/24/2023 11:47:13 AM: [ Training Time!0 hours 0 minutes 44 seconds ]\n"
          ]
        }
      ],
      "source": [
        "main(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EB_6kKpGPD5V"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}