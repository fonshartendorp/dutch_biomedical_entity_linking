{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtzGeiFqMmmD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from pytorch_metric_learning import samplers\n",
    "import logging\n",
    "import time\n",
    "import pdb\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9qk5bBgNI25"
   },
   "outputs": [],
   "source": [
    "from src.data_loader import (\n",
    "    DictionaryDataset,\n",
    "    QueryDataset,\n",
    "    QueryDataset_pretraining,\n",
    "    MetricLearningDataset,\n",
    "    MetricLearningDataset_pairwise,\n",
    ")\n",
    "from src.model_wrapper import (\n",
    "    Model_Wrapper\n",
    ")\n",
    "from src.metric_learning import (\n",
    "    Sap_Metric_Learning,\n",
    ")\n",
    "\n",
    "LOGGER = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxYnZi-rQZuK"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'train_dir' : dataset_dir,\n",
    "    'model_dir' : input_model_directory_path,\n",
    "    'output_dir' : output_model_directory_path,\n",
    "    'use_cuda' : True,\n",
    "    'learning_rate' : 0.0001,\n",
    "    'weight_decay' : 0.01,\n",
    "    'max_length' : 25,\n",
    "    'train_batch_size' : 512,\n",
    "    'epoch' : 10,\n",
    "    'checkpoint_step' : 10000000,\n",
    "    'pairwise' : True,\n",
    "    'amp' : True,\n",
    "    'random_seed' : 1993,\n",
    "    'loss' : 'ms_loss',\n",
    "    'use_miner' : True,\n",
    "    'miner_margin' : 0.2,\n",
    "    'type_of_triplets' : 'all',\n",
    "    'agg_mode' : 'cls',\n",
    "    'save_checkpoint_all' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evBrm6QWOHgc"
   },
   "outputs": [],
   "source": [
    "def init_logging():\n",
    "    LOGGER.setLevel(logging.INFO)\n",
    "    fmt = logging.Formatter('%(asctime)s: [ %(message)s ]',\n",
    "                            '%m/%d/%Y %I:%M:%S %p')\n",
    "    console = logging.StreamHandler()\n",
    "    console.setFormatter(fmt)\n",
    "    LOGGER.addHandler(console)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mttHiZ1KOJeE"
   },
   "outputs": [],
   "source": [
    "def init_seed(seed=None):\n",
    "    if seed is None:\n",
    "        seed = int(round(time.time() * 1000)) % 10000\n",
    "\n",
    "    LOGGER.info(\"Using seed={}, pid={}\".format(seed, os.getpid()))\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xl-gHDzQOL0U"
   },
   "outputs": [],
   "source": [
    "def load_dictionary(dictionary_path):\n",
    "    \"\"\"\n",
    "    load dictionary\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dictionary_path : str\n",
    "        a path of dictionary\n",
    "    \"\"\"\n",
    "    dictionary = DictionaryDataset(\n",
    "            dictionary_path = dictionary_path\n",
    "    )\n",
    "\n",
    "    return dictionary.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Fmma7FaOYqw"
   },
   "outputs": [],
   "source": [
    "def load_queries(data_dir, filter_composite, filter_duplicate):\n",
    "    \"\"\"\n",
    "    load query data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    is_train : bool\n",
    "        train or dev\n",
    "    filter_composite : bool\n",
    "        filter composite mentions\n",
    "    filter_duplicate : bool\n",
    "        filter duplicate queries\n",
    "    \"\"\"\n",
    "    dataset = QueryDataset(\n",
    "        data_dir=data_dir,\n",
    "        filter_composite=filter_composite,\n",
    "        filter_duplicate=filter_duplicate\n",
    "    )\n",
    "\n",
    "    return dataset.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOaq0wZRObP4"
   },
   "outputs": [],
   "source": [
    "def load_queries_pretraining(data_dir, filter_duplicate):\n",
    "    \"\"\"\n",
    "    load query data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    is_train : bool\n",
    "        train or dev\n",
    "    filter_composite : bool\n",
    "        filter composite mentions\n",
    "    filter_duplicate : bool\n",
    "        filter duplicate queries\n",
    "    \"\"\"\n",
    "    dataset = QueryDataset_pretraining(\n",
    "        data_dir=data_dir,\n",
    "        filter_duplicate=filter_duplicate\n",
    "    )\n",
    "\n",
    "    return dataset.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HUORGJPOfUr"
   },
   "outputs": [],
   "source": [
    "def train(args, data_loader, model, scaler=None, model_wrapper=None, step_global=0):\n",
    "    LOGGER.info(\"train!\")\n",
    "\n",
    "    train_loss = 0\n",
    "    train_steps = 0\n",
    "    model.cuda()\n",
    "    model.train()\n",
    "    for i, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        model.optimizer.zero_grad()\n",
    "\n",
    "        batch_x1, batch_x2, batch_y = data\n",
    "        batch_x_cuda1, batch_x_cuda2 = {},{}\n",
    "        for k,v in batch_x1.items():\n",
    "            batch_x_cuda1[k] = v.cuda()\n",
    "        for k,v in batch_x2.items():\n",
    "            batch_x_cuda2[k] = v.cuda()\n",
    "\n",
    "        batch_y_cuda = batch_y.cuda()\n",
    "\n",
    "        if args['amp']:\n",
    "            with autocast():\n",
    "                loss = model(batch_x_cuda1, batch_x_cuda2, batch_y_cuda)\n",
    "        else:\n",
    "            loss = model(batch_x_cuda1, batch_x_cuda2, batch_y_cuda)\n",
    "        if args['amp']:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(model.optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            model.optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_steps += 1\n",
    "        step_global += 1\n",
    "\n",
    "        # save model every K iterations\n",
    "        if step_global % args['checkpoint_step'] == 0:\n",
    "            checkpoint_dir = os.path.join(args['output_dir'], \"checkpoint_iter_{}\".format(str(step_global)))\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            model_wrapper.save_model(checkpoint_dir)\n",
    "    train_loss /= (train_steps + 1e-9)\n",
    "    return train_loss, step_global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LKKwXXyO3Zu"
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    init_logging()\n",
    "\n",
    "    torch.manual_seed(args['random_seed'])\n",
    "\n",
    "    # prepare for output\n",
    "    if not os.path.exists(args['output_dir']):\n",
    "        os.makedirs(args['output_dir'])\n",
    "\n",
    "\n",
    "    # load BERT tokenizer, dense_encoder\n",
    "    model_wrapper = Model_Wrapper()\n",
    "\n",
    "    encoder, tokenizer = model_wrapper.load_bert(\n",
    "        path=args['model_dir'],\n",
    "        max_length=25,\n",
    "        use_cuda=True,\n",
    "    )\n",
    "\n",
    "    # load SAP model\n",
    "    model = Sap_Metric_Learning(\n",
    "            encoder = encoder,\n",
    "            learning_rate=args['learning_rate'],\n",
    "            weight_decay=args['weight_decay'],\n",
    "            use_cuda=args['use_cuda'],\n",
    "            pairwise=args['pairwise'],\n",
    "            loss=args['loss'],\n",
    "            use_miner=args['use_miner'],\n",
    "            miner_margin=args['miner_margin'],\n",
    "            type_of_triplets=args['type_of_triplets'],\n",
    "            agg_mode=args['agg_mode'],\n",
    "    )\n",
    "\n",
    "    def collate_fn_batch_encoding(batch):\n",
    "        query1, query2, query_id = zip(*batch)\n",
    "        query_encodings1 = tokenizer.batch_encode_plus(\n",
    "                list(query1),\n",
    "                max_length=args['max_length'],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors=\"pt\")\n",
    "        query_encodings2 = tokenizer.batch_encode_plus(\n",
    "                list(query2),\n",
    "                max_length=args['max_length'],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors=\"pt\")\n",
    "\n",
    "        query_ids = torch.tensor(list(query_id))\n",
    "        return  query_encodings1, query_encodings2, query_ids\n",
    "\n",
    "    if args['pairwise']:\n",
    "        train_set = MetricLearningDataset_pairwise(\n",
    "                path=args['train_dir'],\n",
    "                tokenizer = tokenizer\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_set,\n",
    "            batch_size=args['train_batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=16,\n",
    "            collate_fn=collate_fn_batch_encoding\n",
    "        )\n",
    "    else:\n",
    "        train_set = MetricLearningDataset(\n",
    "            path=args['train_dir'],\n",
    "            tokenizer = tokenizer\n",
    "        )\n",
    "        # using a sampler\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_set,\n",
    "            batch_size=args['train_batch_size'],\n",
    "            #shuffle=True,\n",
    "            sampler=samplers.MPerClassSampler(train_set.query_ids,\\\n",
    "                2, length_before_new_iter=100000),\n",
    "            num_workers=16,\n",
    "            )\n",
    "    # mixed precision training\n",
    "    if args['amp']:\n",
    "        scaler = GradScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    start = time.time()\n",
    "    step_global = 0\n",
    "    for epoch in range(1,args['epoch']+1):\n",
    "        # embed dense representations for query and dictionary for train\n",
    "        # Important! This is iterative process because dense represenation changes as model is trained.\n",
    "        LOGGER.info(\"Epoch {}/{}\".format(epoch,args['epoch']))\n",
    "\n",
    "        # train\n",
    "        train_loss, step_global = train(args, data_loader=train_loader, model=model, scaler=scaler, model_wrapper=model_wrapper, step_global=step_global)\n",
    "        LOGGER.info('loss/train_per_epoch={}/{}'.format(train_loss,epoch))\n",
    "\n",
    "        # save model every epoch\n",
    "        if args['save_checkpoint_all']:\n",
    "            checkpoint_dir = os.path.join(args['output_dir'], \"checkpoint_{}\".format(epoch))\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            model_wrapper.save_model(checkpoint_dir)\n",
    "\n",
    "        # save model last epoch\n",
    "        if epoch == args['epoch']:\n",
    "            model_wrapper.save_model(args['output_dir'])\n",
    "\n",
    "    end = time.time()\n",
    "    training_time = end-start\n",
    "    training_hour = int(training_time/60/60)\n",
    "    training_minute = int(training_time/60 % 60)\n",
    "    training_second = int(training_time % 60)\n",
    "    LOGGER.info(\"Training Time!{} hours {} minutes {} seconds\".format(training_hour, training_minute, training_second))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZcl7nZ-O4yJ",
    "outputId": "3d3dd00e-c079-4728-dd29-68d3bf8b9767"
   },
   "outputs": [],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EB_6kKpGPD5V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
